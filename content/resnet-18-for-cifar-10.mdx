---
title: "Resnet 18 implementation for CIFAR10"
publishedAt: "22-05-2025"
summary: "Resnet 18 implementation"
---
## What is a resnet and why use a resnet?
In this project I built a resnet18 which was trained on a dataset of 50,000 32x32 rgb images.

One problem with training deep neural networks is that as networks get bigger they can begin to become unstable leading to worse accuracy than a shallower network, called degradation.
Residual networks use residual blocks with skip connections, instead of a full transformation in each block the network learns a residual (the difference between the input and output).
Resnet18 is a popular version using 18 layers , in groups of 4 residual blocks.

## Setup
```py
import torch
import torchvision
import torchvision.datasets as datasets
import torchvision.transforms as transforms
import torch.nn as nn
from torch.utils.data import DataLoader, random_split
import torch.optim as optim

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
```
Importing everything needed from pytorch and making sure it can run on either cpu or cuda (nvidia gpus) without having to change code.
## Data preprocessing and loading
```py
ean = [0.4914, 0.4822, 0.4465]
std  = [0.2023, 0.1994, 0.2010]

transform_train = transforms.Compose([
    transforms.RandomCrop(32, padding=4),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize(mean, std)
])
transform_test = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(mean, std)
])

train_dataset_CIFAR10 = datasets.CIFAR10(root='./data',
                                         train=True,
                                         download=True,
                                         transform=transform_train)
test_dataset_CIFAR10 = datasets.CIFAR10(root='./data',
                                        train=False,
                                        download=True,
                                        transform=transform_test)
train_dataloader = DataLoader(train_dataset_CIFAR10,
                              batch_size=128,
                              shuffle=True,
                              num_workers=2)
test_dataloader = DataLoader(test_dataset_CIFAR10,
                             batch_size=100,
                             shuffle=False,
                             num_workers=2)
```
I used data augmentation, random crop and horizontal flip to improve model generalisation (better at predicting unseen data).
The images are then normalised to the CIFAR10 mean and standard deviation. The data is then batched to decrease train time

## Basic residual block
```python
class BasicBlock(nn.Module):
  def __init__(self, in_channels, out_channels, stride=1):
    super(BasicBlock, self).__init__()
    self.conv1 = nn.Conv2d(in_channels,
                           out_channels,
                           kernel_size=3,
                           stride=stride,
                           padding=1,
                           bias=False)
    self.bn1 = nn.BatchNorm2d(out_channels)
    self.relu = nn.ReLU(inplace=True)
    self.conv2 = nn.Conv2d(out_channels,
                           out_channels,
                           kernel_size=3,
                           stride=1,
                           padding=1,
                           bias=False)
    self.bn2 = nn.BatchNorm2d(out_channels)

    self.shortcut = nn.Sequential()
    if stride != 1 or in_channels != out_channels:
        self.shortcut = nn.Sequential(
            nn.Conv2d(in_channels,
                      out_channels,
                      kernel_size=1,
                      stride=stride,
                      bias=False),
            nn.BatchNorm2d(out_channels)
        )

  def forward(self,x):
    out = self.conv1(x)
    out = self.bn1(out)
    out = self.relu(out)
    out = self.conv2(out)
    out = self.bn2(out)
    out += self.shortcut(x)
    out = self.relu(out)
    return out
```
